{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894ad88d",
   "metadata": {},
   "source": [
    "# Neural ODEs\n",
    "\n",
    "We here consider, as already done in the [Neural Hamiltonian ODE](<./NeuralHamiltonianODEs.ipynb>) example, a generic system in the form:\n",
    "\n",
    "$$\n",
    "\\dot {\\mathbf x} = \\mathbf f(\\mathbf x, \\mathcal N_\\theta(\\mathbf x))\n",
    "$$\n",
    "\n",
    "whose solution is indicated with $\\mathbf x(t; x_0, \\theta)$ to explicitly denote the dependance on the initial conditions $\\mathbf x+0$ and the network parameters $\\theta$.\n",
    "We refer to these systems as Neural ODEs, a term that has been made popular in the paper,\n",
    "\n",
    "*Chen, Ricky TQ, Yulia Rubanova, Jesse Bettencourt, and David K. Duvenaud.* \"Neural ordinary differential equations.\" Advances in neural information processing systems 31 (2018).\n",
    "\n",
    "where it has been used to indicate a specific form of the above equation, when the state represented the neuronal activity of a Neural Network. We here depart from that terminology and use the term in general for any ODE with a right hand side containing an Artificial Neural Network. All the cases illustrated in the [Neural Hamiltonian ODE](<./NeuralHamiltonianODEs.ipynb>) example are, therefore, also to be considered as special cases of Neural ODEs.\n",
    "\n",
    "Whenever we have a Neural ODE, it is important to be able to define a training pipeline able to change the neural parameters $\\theta$ as to make some loss decrease. \n",
    "\n",
    "We indicate such a loss with $\\mathcal L(\\mathbf x(t; x_0, \\theta))$ and show in this example how to compute, using *heyoka*, its gradient, and hence how to setup a training pipeline for Neural ODEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7030234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The usual main imports\n",
    "import heyoka as hy\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients we seek can be written as:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\mathbf x_0} =  \\frac{\\partial \\mathbf x}{\\partial \\mathbf x_0} \\frac{\\partial \\mathcal L}{\\partial \\mathbf x}\\\\\n",
    "\\frac{\\partial \\mathcal L}{\\partial \\theta} = \\frac{\\partial \\mathbf x}{\\partial \\theta} \\frac{\\partial \\mathcal L}{\\partial \\mathbf x}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In the expressions above we know the functional form of $\\mathcal L$ and hence its derivatives w.r.t. $\\mathbf x$, we thus need to compute the remaining terms, i.e. the ODE sensitivities: $\\mathbf \\Phi = \\frac{\\partial \\mathbf x(t)}{\\partial \\mathbf x_0} $, $\\boldsymbol \\varphi = \\frac{\\partial \\mathbf x(t)}{\\partial \\boldsymbol \\theta} $\n",
    "\n",
    "```{note}\n",
    "\n",
    "The computation of the ODE sensitivities can be achieved following two methods: the variational equations and the adjoint method. Both methods compute the same quantities and we shall see how they are, ultimately, two version of the same reasoning leading to algorithms sharing a similar complexity, contrary to what sometimes believed / reported in the scientific literature.\n",
    "```\n",
    "\n",
    "For the sake of clarity we here consider a system in the simplified form:\n",
    "\n",
    "$$\n",
    "\\dot {\\mathbf x} = \\mathcal N_\\theta(\\mathbf x)\n",
    "$$\n",
    "\n",
    "the r.h.s. is a Feed Forward Neural Network and we use the *heyoka* factory function `ffnn()` to instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tanh((p85 + (p60 * tanh((p80 + (p35 * tanh((p75 + (p10 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p11 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p12 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p13 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p14 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p36 * tanh((p76 + (p15 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p16 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p17 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p18 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p19 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p37 * tanh((p77 + (p20 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p21 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p22 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p23 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p24 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p38 * tanh((p78 + (p25 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p26 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p27 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p28 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p29 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p39 * ..., tanh((p86 + (p65 * tanh((p80 + (p35 * tanh((p75 + (p10 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p11 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p12 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p13 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p14 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p36 * tanh((p76 + (p15 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p16 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p17 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p18 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p19 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p37 * tanh((p77 + (p20 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p21 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p22 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p23 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p24 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p38 * tanh((p78 + (p25 * tanh((p70 + (p0 * x) + (p1 * y)))) + (p26 * tanh((p71 + (p2 * x) + (p3 * y)))) + (p27 * tanh((p72 + (p4 * x) + (p5 * y)))) + (p28 * tanh((p73 + (p6 * x) + (p7 * y)))) + (p29 * tanh((p74 + (p8 * x) + (p9 * y))))))) + (p39 * ...]\n"
     ]
    }
   ],
   "source": [
    "# We create the symbols for the network inputs (only one in this frst simple case)\n",
    "state = hy.make_vars(\"x\", \"y\")\n",
    "\n",
    "# We define as nonlinearity a simple linear layer\n",
    "linear = lambda inp: inp\n",
    "\n",
    "# We call the factory to construct the FFNN:\n",
    "ffnn = hy.model.ffnn(inputs = state, nn_hidden = [5,5,5], n_out = 2, activations = [hy.tanh, hy.tanh, hy.tanh, hy.tanh])\n",
    "print(ffnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Variational Equations\n",
    "As derived already in the examples dedicated to the [variational equations](<./The Variational equations.ipynb>) and to the [Periodic orbits in the CR3BP](<./The Variational equations.ipynb>) the ODE sensitivities can be computed from the differential equations:\n",
    "\n",
    "$$\n",
    " \\frac{d\\mathbf \\Phi}{dt} = \\nabla_\\mathbf x \\mathcal N_\\theta(\\mathbf x) \\cdot \\mathbf \\Phi \\qquad (n,n) = (n,n) (n,n)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\frac{d\\boldsymbol \\varphi}{dt} = \\nabla_\\mathbf x \\mathcal N_\\theta(\\mathbf x) \\cdot \\boldsymbol \\varphi + \\frac{\\partial \\mathcal N_\\theta(\\mathbf x)}{\\partial \\boldsymbol \\theta} \\qquad (n,N) = (n,n) (n,N) + (n,N) \n",
    "$$\n",
    "where we have reported also the dimensions of the various terms for clarity: $n$ is the system dimension (2 in our case) and $N$ the number of parameters (87 in our case).\n",
    "\n",
    "Now this may all sound very complicated, but *heyoka* simplifies everything for you, so that the code looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dNdtheta: (2, 87)\n",
      "Shape of dNdx: (2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Parametes\n",
    "dNdtheta = hy.diff_tensors(ffnn, hy.diff_args.params)\n",
    "dNdtheta = dNdtheta.jacobian\n",
    "print(\"Shape of dNdtheta:\", dNdtheta.shape)\n",
    "\n",
    "# Variables\n",
    "dNdx = hy.diff_tensors(ffnn, hy.diff_args.vars)\n",
    "dNdx= dNdx.jacobian\n",
    "print(\"Shape of dNdx:\", dNdx.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assemble the differential equation we must now define the symbolic variables of all the elements in $\\mathbf \\Phi$ and $\\mathbf p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the symbols for phi\n",
    "symbols_phi = []\n",
    "for i in range(dNdtheta.shape[0]):\n",
    "    for j in range(dNdtheta.shape[0]):\n",
    "        # Here we define the symbol for the variations\n",
    "        symbols_phi.append(\"phi_\"+str(i)+str(j))  \n",
    "phi = np.array(hy.make_vars(*symbols_phi)).reshape((dNdtheta.shape[0], dNdtheta.shape[0]))\n",
    "\n",
    "# We define the symbols for varphi\n",
    "symbols_varphi = []\n",
    "for i in range(dNdtheta.shape[0]):\n",
    "    for j in range(dNdtheta.shape[1]):\n",
    "        # Here we define the symbol for the variations\n",
    "        symbols_varphi.append(\"varphi_\"+str(i)+str(j))  \n",
    "varphi = np.array(hy.make_vars(*symbols_varphi)).reshape((dNdtheta.shape[0], dNdtheta.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and finally assemble the variational equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The (variational) equations of motion in matrix form\n",
    "dphidt = dNdx@phi\n",
    "dvarphidt =  dNdx@varphi + dNdtheta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains to be done, is to build a Tayor adaptive integrator (an ODE solver) with all the compiled equations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dyn = []\n",
    "# The \\dot x = ffnn\n",
    "for lhs, rhs in zip(state,ffnn):\n",
    "    dyn.append((lhs, rhs))\n",
    "# The variational equations for x0\n",
    "for lhs, rhs in zip(phi.flatten(),dphidt.flatten()):\n",
    "    dyn.append((lhs, rhs))\n",
    "# The variaitonal equations for the thetas\n",
    "for lhs, rhs in zip(varphi.flatten(),dvarphidt.flatten()):\n",
    "    dyn.append((lhs, rhs))\n",
    "# These are the initial conditions on the variational equations (the identity matrix) and zeros \n",
    "ic_var = np.eye(len(state)).flatten().tolist() + [0.] * len(symbols_varphi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.4344515800476074 seconds --- to build the Taylor integrator\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ta = hy.taylor_adaptive(\n",
    "    # The ODEs.\n",
    "    dyn,\n",
    "    # The initial conditions.\n",
    "    [1.1, 1.1] + ic_var,\n",
    "    # Operate in compact mode.\n",
    "    compact_mode = True\n",
    ")\n",
    "print(\"--- %s seconds --- to build the Taylor integrator\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0006282329559326172 seconds --- to propagate\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ta.propagate_until(10.)\n",
    "print(\"--- %s seconds --- to propagate\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the Adjoint Method\n",
    "Let us, for a moment, instead of seeking $\\frac{\\partial \\mathbf x(t)}{\\partial \\mathbf x_0}$, seek the opposite, and thus define:\n",
    "\n",
    "$$\n",
    "\\mathbf a = \\frac{\\partial \\mathbf x_0}{\\partial \\mathbf x(t)} \n",
    "$$\n",
    "\n",
    "by definition $\\mathbf a$ is the inverse of $\\mathbf \\Phi$, which implies $\\mathbf a = \\mathbf \\Phi^{-1}$ and thus we also have (accounting fo the fact that the derivative of a matrix inverse is $\\frac{d\\mathbf A^{-1}}{dt} = - \\mathbf A^{-1}\\frac{d \\mathbf A}{dt}\\mathbf A^{-1}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathbf a}{\\partial t} = - \\mathbf \\Phi^{-1} \\frac{\\partial \\mathbf \\Phi}{\\partial t} \\mathbf \\Phi^{-1} =- \\mathbf \\Phi^{-1} \\nabla_\\mathbf x \\mathcal N_\\theta(\\mathbf x)  \\mathbf \\Phi  \\mathbf \\Phi^{-1} = -\\mathbf a \\nabla_\\mathbf x \\mathcal N_\\theta(\\mathbf x)\n",
    "$$\n",
    "\n",
    "which is a very compact and elegant demonstration (I know right?) of the adjoint equation for our case, otherwise often derived using the calculus of variations and a much more lengthy sequence of variational identities. \n",
    "\n",
    "More importantly the derivation shows how the adjoint method is strongly related to the variational equations and thus the resulting algorithm complexity cannot, and will not be different.\n",
    "\n",
    "In the classic derivation of the adjoint method the sensitivities are taken with respect to $\\mathbf x(T)$ and not $\\mathbf x_0 = \\mathbf x(t_0)$. This is irrelevant for the purpose of the demonstration as $t_0$ is just a point in time and can represent a point in the future as well as a point in the past.\n",
    "\n",
    "In the paper \"Neural ordinary differential equations.\" which popularized the use of ffnn on the r.h.s od ODEs, the derivation is made for a loss $\\mathcal L$, and and ODE is seeked for $\\mathbf {\\hat a} = \\frac{\\partial \\mathcal L(\\mathbf x(T))}{\\partial \\mathbf x(t)}$.\n",
    " \n",
    "Since:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf {\\hat a} = \\frac{\\partial \\mathcal L(\\mathbf x(T))}{\\partial \\mathbf x(t)} = \\frac{\\partial \\mathcal L(\\mathbf x(T))}{\\partial \\mathbf x(T)}\\frac{\\partial \\mathbf x(T)}{\\partial \\mathbf x(t)}\n",
    "$$\n",
    "\n",
    "Its easy to see that the same differential equation we proved above holds for $\\mathbf {\\hat a}$ by taking the time derivatoive of the above identity and noting that $\\frac{\\partial \\mathcal L(\\mathbf x(T))}{\\partial \\mathbf x(T)}$ is a constant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
